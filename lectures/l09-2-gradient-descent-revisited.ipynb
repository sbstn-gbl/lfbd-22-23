{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Revisited\n",
    "\n",
    "This notebook revisits inference for a ___linear model___ using gradient descent.\n",
    "\n",
    "Our focus will be using `pytorch` to make the implementation more parsimonious:\n",
    "1. Generate data for a linear model\n",
    "1. Review gradient descent, implement inference in `numpy`\n",
    "1. Implement inference using PyTorch's autograd\n",
    "1. Leverage `torch.nn` module to simplify the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(x, label):\n",
    "    x = [xi.item() if torch.is_tensor(xi) else xi for xi in x]\n",
    "    print(\n",
    "        f\"{label+':':9s} y = {x[0]:.2f} + {x[1]:.2f} x_1 + {x[2]:.2f} x_2 + {x[3]:.2f} x_3\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Our data generating process is a linear model\n",
    "\n",
    "$\n",
    "\\quad \\mathbf y = a + b\\,\\mathbf x_1 + c\\,\\mathbf x_2 + d\\,\\mathbf x_3 + \\mathbf\\varepsilon\n",
    "$\n",
    "\n",
    "We'll set\n",
    "\n",
    "- $a = 3$\n",
    "- $b = -2$\n",
    "- $c = 1$\n",
    "- $d = -1$\n",
    "- $\\varepsilon \\sim \\mathcal{N}(\\mu_{\\varepsilon}, \\sigma_{\\varepsilon})$ with $\\mu_{\\varepsilon}=0$ and $\\sigma_{\\varepsilon}=1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(501)\n",
    "\n",
    "N = 1_000\n",
    "\n",
    "at = 3\n",
    "bt = -2\n",
    "ct = 1\n",
    "dt = -1\n",
    "\n",
    "sigma_err = 1\n",
    "\n",
    "err = np.random.normal(0, sigma_err, N)\n",
    "x1 = np.random.normal(2, 1, N)\n",
    "x2 = np.random.normal(3, 1, N)\n",
    "x3 = np.random.normal(1, 1, N)\n",
    "y = at + bt * x1 + ct * x2 + dt * x3 + err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual implementation with `numpy`\n",
    "\n",
    "1. Manually implement loss function\n",
    "1. Manually calculate gradient\n",
    "1. Manually update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(501)\n",
    "np.random.seed(501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9999 | loss = 0.619665\n",
      "   19999 | loss = 0.537811\n",
      "   29999 | loss = 0.513246\n",
      "   39999 | loss = 0.505873\n",
      "   49999 | loss = 0.503661\n",
      "   59999 | loss = 0.502997\n",
      "   69999 | loss = 0.502798\n",
      "   79999 | loss = 0.502738\n",
      "   89999 | loss = 0.502720\n",
      "   99999 | loss = 0.502714\n",
      "\n",
      "Result\n",
      "True:     y = 3.00 + -2.00 x_1 + 1.00 x_2 + -1.00 x_3\n",
      "Manual:   y = 3.19 + -2.03 x_1 + 0.94 x_2 + -0.97 x_3\n"
     ]
    }
   ],
   "source": [
    "# define learning rate\n",
    "learning_rate = 1e-3\n",
    "n_epochs = 100_000\n",
    "\n",
    "# random starting values\n",
    "am, bm, cm, dm = np.random.uniform(-1, 1, 4)\n",
    "\n",
    "# training: looping over epochs\n",
    "for e in range(n_epochs):\n",
    "\n",
    "    y_pred = am + bm * x1 + cm * x2 + dm * x3\n",
    "\n",
    "    loss = np.mean((y_pred - y) ** 2 / 2)\n",
    "\n",
    "    if e % 10_000 == 9_999:\n",
    "        print(f\"{e:8d} | loss = {loss:.6f}\")\n",
    "\n",
    "    grad_y_pred = y_pred - y\n",
    "\n",
    "    grad_a = (grad_y_pred * 1).mean()\n",
    "    grad_b = (grad_y_pred * x1).mean()\n",
    "    grad_c = (grad_y_pred * x2).mean()\n",
    "    grad_d = (grad_y_pred * x3).mean()\n",
    "\n",
    "    am -= learning_rate * grad_a  # am = am - learning_rate * grad_a\n",
    "    bm -= learning_rate * grad_b\n",
    "    cm -= learning_rate * grad_c\n",
    "    dm -= learning_rate * grad_d\n",
    "\n",
    "print(f\"\\nResult\")\n",
    "print_result([at, bt, ct, dt], \"True\")\n",
    "print_result([am, bm, cm, dm], \"Manual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `autograd` from `PyTorch`\n",
    "\n",
    "1. Manually implement loss function\n",
    "1. Use `torch` gradients\n",
    "1. Manually update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(501)\n",
    "np.random.seed(501)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some first steps\n",
    "\n",
    "#### Turn `numpy` arrays into `torch` tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1t = torch.tensor(x1)\n",
    "x2t = torch.tensor(x2)\n",
    "x3t = torch.tensor(x3)\n",
    "yt = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((), requires_grad=True)\n",
    "b = torch.randn((), requires_grad=True)\n",
    "c = torch.randn((), requires_grad=True)\n",
    "d = torch.randn((), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2067, -3.6361, -5.1370, -4.8801, -0.3303, -0.8349, -5.3536, -2.8600,\n",
       "        -0.6940, -3.4232], dtype=torch.float64, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_pred = a + b * x1t + c * x2t + d * x3t\n",
    "yt_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.8336, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.mean((yt_pred - yt).pow(2) / 2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.3725762367248535, -3.372576336569959)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad.item(), (yt_pred - yt).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.728719234466553, -4.728719281632131)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad.item(), ((yt_pred - yt) * x1t).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-12.700126647949219, -12.700126466714602)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad.item(), ((yt_pred - yt) * x2t).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.2989070415496826, -1.2989070407896441)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.grad.item(), ((yt_pred - yt) * x3t).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(501)\n",
    "np.random.seed(501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9999 | loss = 0.541866\n",
      "   19999 | loss = 0.514465\n",
      "   29999 | loss = 0.506240\n",
      "   39999 | loss = 0.503772\n",
      "   49999 | loss = 0.503031\n",
      "   59999 | loss = 0.502808\n",
      "   69999 | loss = 0.502741\n",
      "   79999 | loss = 0.502721\n",
      "   89999 | loss = 0.502715\n",
      "   99999 | loss = 0.502713\n",
      "\n",
      "Result\n",
      "True:     y = 3.00 + -2.00 x_1 + 1.00 x_2 + -1.00 x_3\n",
      "Manual:   y = 3.19 + -2.03 x_1 + 0.94 x_2 + -0.97 x_3\n",
      "PyTorch:  y = 3.19 + -2.03 x_1 + 0.94 x_2 + -0.97 x_3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "n_epochs = 100_000\n",
    "\n",
    "ap = torch.randn((), requires_grad=True)\n",
    "bp = torch.randn((), requires_grad=True)\n",
    "cp = torch.randn((), requires_grad=True)\n",
    "dp = torch.randn((), requires_grad=True)\n",
    "\n",
    "for e in range(n_epochs):\n",
    "\n",
    "    # forward\n",
    "    yt_pred = ap + bp * x1t + cp * x2t + dp * x3t\n",
    "    loss = torch.mean((yt_pred - yt).pow(2) / 2)\n",
    "    if e % 10_000 == 9_999:\n",
    "        print(f\"{e:8d} | loss = {loss.item():.6f}\")\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ap -= learning_rate * ap.grad\n",
    "        bp -= learning_rate * bp.grad\n",
    "        cp -= learning_rate * cp.grad\n",
    "        dp -= learning_rate * dp.grad\n",
    "\n",
    "        ap.grad = None\n",
    "        bp.grad = None\n",
    "        cp.grad = None\n",
    "        dp.grad = None\n",
    "\n",
    "print(f\"\\nResult\")\n",
    "print_result([at, bt, ct, dt], \"True\")\n",
    "print_result([am, bm, cm, dm], \"Manual\")\n",
    "print_result([ap, bp, cp, dp], \"PyTorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `torch.nn`\n",
    "\n",
    "1. Use `torch` loss function\n",
    "1. Use `torch` gradients\n",
    "1. Use `torch` parameter update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(501)\n",
    "np.random.seed(501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.tensor(np.column_stack([x1, x2, x3]).astype(np.float32))\n",
    "yt = torch.tensor(y.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(torch.nn.Linear(3, 1), torch.nn.Flatten(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9999 | loss = 0.563865\n",
      "   19999 | loss = 0.521067\n",
      "   29999 | loss = 0.508222\n",
      "   39999 | loss = 0.504367\n",
      "   49999 | loss = 0.503209\n",
      "   59999 | loss = 0.502862\n",
      "   69999 | loss = 0.502757\n",
      "   79999 | loss = 0.502726\n",
      "   89999 | loss = 0.502716\n",
      "   99999 | loss = 0.502713\n",
      "\n",
      "Result\n",
      "True:     y = 3.00 + -2.00 x_1 + 1.00 x_2 + -1.00 x_3\n",
      "Manual:   y = 3.19 + -2.03 x_1 + 0.94 x_2 + -0.97 x_3\n",
      "PyTorch:  y = 3.19 + -2.03 x_1 + 0.94 x_2 + -0.97 x_3\n",
      "PyTorch2: y = 3.19 + -2.03 x_1 + 0.94 x_2 + -0.97 x_3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "n_epochs = 100_000\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for e in range(n_epochs):\n",
    "\n",
    "    # forward\n",
    "    yt_pred = model(xt)\n",
    "    loss = loss_fn(yt_pred, yt) / 2\n",
    "\n",
    "    if e % 10_000 == 9_999:\n",
    "        print(f\"{e:8d} | loss = {loss.item():.6f}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    # udpate\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"\\nResult\")\n",
    "print_result([at, bt, ct, dt], \"True\")\n",
    "print_result([am, bm, cm, dm], \"Manual\")\n",
    "print_result([ap, bp, cp, dp], \"PyTorch\")\n",
    "print_result(torch.cat([model[0].bias, model[0].weight.flatten()]), \"PyTorch2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<b>Learning from Big Data</b> <br>\n",
    "Sebastian Gabel <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Inspired by this [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Self Study) The backpropagation algorithm\n",
    "\n",
    "The notebook includes the following steps:\n",
    "1. Generate data and compute gradients with PyTorch\n",
    "1. Implement (dense linear) layers and activation functions (including local gradients)\n",
    "1. Implement forward pass\n",
    "1. Implement backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2a(x):\n",
    "    return x.detach().numpy()\n",
    "\n",
    "def test(arr, tens, eps=1e-6):\n",
    "    assert np.max(np.abs(arr - t2a(tens))) < eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = np.random.uniform(-1, 1, size=(8, 2))\n",
    "xt_in = torch.Tensor(x_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.choice([0, 1], size=(8, 1))\n",
    "yt = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "### Set seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(501)\n",
    "np.random.seed(501)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build net in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h0 = torch.nn.Linear(2, 4)\n",
    "        self.h1 = torch.nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        xb = self.h0(xb)\n",
    "        xb = torch.nn.functional.relu(xb)\n",
    "        xb = self.h1(xb)\n",
    "        return xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_net = Net()\n",
    "net1 = copy.deepcopy(raw_net)\n",
    "net2 = copy.deepcopy(raw_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.h0.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.5283, -0.5109],\n",
       "         [-0.2843,  0.6360],\n",
       "         [-0.6374,  0.5668],\n",
       "         [-0.3771,  0.6981]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.3359, -0.2572,  0.5822,  0.3144], requires_grad=True))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.h0.weight, net1.h0.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.3399, -0.3500, -0.2764, -0.0239]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.4749], requires_grad=True))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.h1.weight, net1.h1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits1 = net1(xt_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs1 = torch.sigmoid(logits1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6287, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1 = torch.nn.functional.binary_cross_entropy(probs1, yt)\n",
    "loss1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0140,  0.0153],\n",
       "        [-0.0114,  0.0145],\n",
       "        [ 0.0051,  0.0153],\n",
       "        [ 0.0014,  0.0024]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.h0.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0158,  0.0220,  0.0482,  0.0031])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.h0.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0291, -0.0194, -0.1212, -0.0878]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.h1.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1745])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.h1.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `binary_cross_entropy_with_logits` yields same result\n",
    "\n",
    "... and is actually preferred over `binary_cross_entropy`. Care to guess why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1936],\n",
       "        [ 0.5221],\n",
       "        [ 0.3482],\n",
       "        [ 0.3536],\n",
       "        [ 0.3307],\n",
       "        [-0.0119],\n",
       "        [ 0.3099],\n",
       "        [ 0.3991]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits2 = net2(xt_in)\n",
    "logits2.retain_grad()  # so we can compare even non leaf tensors\n",
    "logits2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6287, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = torch.nn.functional.binary_cross_entropy_with_logits(logits2, yt)\n",
    "loss2.retain_grad()\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0140,  0.0153],\n",
       "         [-0.0114,  0.0145],\n",
       "         [ 0.0051,  0.0153],\n",
       "         [ 0.0014,  0.0024]]),\n",
       " tensor([[ 0.0140,  0.0153],\n",
       "         [-0.0114,  0.0145],\n",
       "         [ 0.0051,  0.0153],\n",
       "         [ 0.0014,  0.0024]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2.h0.weight.grad, net1.h0.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0158,  0.0220,  0.0482,  0.0031]),\n",
       " tensor([-0.0158,  0.0220,  0.0482,  0.0031]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2.h0.bias.grad, net1.h0.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0291, -0.0194, -0.1212, -0.0878]]),\n",
       " tensor([[-0.0291, -0.0194, -0.1212, -0.0878]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2.h1.weight.grad, net1.h1.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1745]), tensor([-0.1745]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2.h1.bias.grad, net1.h1.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual implementation\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sbstn-gbl/learning-from-big-data/master/source/_static/img/backpropagation.png\" align=\"left\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, shape, activation=\"relu\"):\n",
    "        self.shape = shape\n",
    "        # He initialization\n",
    "        gain = math.sqrt(2) if activation == \"relu\" else 1\n",
    "        std = gain / math.sqrt(self.shape[0])\n",
    "        bound = math.sqrt(3.0) * std\n",
    "        self.weights = np.random.uniform(-bound, bound, self.shape)\n",
    "        bound = 1 / math.sqrt(self.shape[0])\n",
    "        self.bias = np.random.uniform(-bound, bound, self.shape[1])\n",
    "        # store data on forward pass ...\n",
    "        self.input = None\n",
    "        # ... and gradients on backward pass\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None\n",
    "\n",
    "    def forward_pass(self, x, training=True):\n",
    "        self.input = x\n",
    "        return x.dot(self.weights) + self.bias\n",
    "\n",
    "    def backward_pass(self, cum_grad):\n",
    "        weights = self.weights.copy()\n",
    "        self.grad_weights = self.input.T.dot(cum_grad)\n",
    "        self.grad_bias = np.sum(cum_grad, axis=0, keepdims=True)\n",
    "        # we only want to understand backpropagation here so we skip updating the weights\n",
    "        return cum_grad.dot(weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.grad = None\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        self.input = x\n",
    "        return self._sigmoid(x)\n",
    "\n",
    "    def backward_pass(self, cum_grad):\n",
    "        self.grad = self.gradient(self.input)\n",
    "        return cum_grad * self.grad\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.grad = None\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        self.input = x\n",
    "        return (x > 0) * x\n",
    "\n",
    "    def backward_pass(self, cum_grad):\n",
    "        self.grad = self.gradient(self.input)\n",
    "        return cum_grad * self.grad\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return (x > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0 = LinearLayer((2, 4))\n",
    "# overwrite weights with pytorch net initalization so we can compare results\n",
    "l0.weights = raw_net.h0.weight.detach().T.numpy()\n",
    "l0.bias = raw_net.h0.bias.detach().numpy()\n",
    "\n",
    "l1 = LinearLayer((4, 1))\n",
    "# overwrite weights with pytorch net initalization so we can compare results\n",
    "l1.weights = raw_net.h1.weight.detach().T.numpy()\n",
    "l1.bias = raw_net.h1.bias.detach().numpy()\n",
    "\n",
    "a0 = ReLU()\n",
    "a1 = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling the full network\n",
    "layers = [\n",
    "    l0,\n",
    "    a0,\n",
    "    l1,\n",
    "    a1,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = x_in.copy()\n",
    "for layer in layers:\n",
    "    out = layer.forward_pass(out)\n",
    "probs_np = out\n",
    "loss_np = sklearn.metrics.log_loss(y, probs_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.max(np.abs(probs_np - t2a(probs1))) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.max(np.abs(loss_np - t2a(loss1))) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27670447],\n",
       "       [-0.1991569 ],\n",
       "       [-0.21324057],\n",
       "       [-0.21276838],\n",
       "       [-0.21479973],\n",
       "       [-0.25149278],\n",
       "       [-0.2166928 ],\n",
       "       [ 0.31130692]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = -(y / probs_np) + (1 - y) / (1 - probs_np)\n",
    "grad /= grad.shape[0]\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers[::-1]:\n",
    "    grad = layer.backward_pass(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02906101],\n",
       "       [-0.01942071],\n",
       "       [-0.12120559],\n",
       "       [-0.08780599]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.grad_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0291],\n",
       "        [-0.0194],\n",
       "        [-0.1212],\n",
       "        [-0.0878]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2.h1.weight.grad.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(l1.grad_weights, net2.h1.weight.grad.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(l1.grad_bias, net2.h1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(l0.grad_weights, net2.h0.weight.grad.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(l0.grad_bias, net2.h0.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<b>Learning from Big Data</b> <br>\n",
    "Sebastian Gabel <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
